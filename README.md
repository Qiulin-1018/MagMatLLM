# MagMatLLM: LLMâ€‘Guided Genetic Search for Magnetic Insulators

> Accelerated discovery of magnetic insulators via large language models + evolutionary search + firstâ€‘principles validation.

<p align="center">
  <img src="docs/figures/pipeline.png" alt="Pipeline diagram" width="720" />
</p>

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](#)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](#license)
[![Reproducibility](https://img.shields.io/badge/reproducible-yes-brightgreen.svg)](#reproducing-our-results)

---

## âœ¨ Overview

**MagMatLLM** is a closedâ€‘loop framework for discovering **magnetic insulators**. It integrates:

* **LLM** generation and variation of crystal structures (compose / permute / distort);
* **Genetic Algorithms (GA)** for multiâ€‘objective selection across generations;
* **ML surrogate** (e.g., CHGNet) for fast screening of stability and properties;
* **DFT** (phonons, bands, DOS) for definitive validation.

The framework discovers previously unreported candidates under modest compute budgets and validates exemplars with finite band gaps, nonâ€‘zero total magnetic moments, and phonon stability.

> Paper (preprint): *Accelerated Discovery of Magnetic Insulators via LLMâ€‘Guided Genetic Algorithm and Firstâ€‘Principles Validation* (NeurIPS 2025 submission).

---

## ğŸ§± Key Features

* **LLM + GA**: Iterative generation & optimization for multiple objectives (stability, bulk modulus, total magnetic moment, etc.).
* **Two objective modes**: Weightedâ€‘sum and lexicographicâ€‘withâ€‘thresholds.
* **Data augmentation**: Structureâ€‘level augmentations (e.g., AugLiChem) to combat magnetic data scarcity.
* **Multiâ€‘fidelity validation**: CHGNet relax & screen â†’ DFT phonons/bands/DOS for final confirmation.
* **Extensible**: Portable to other propertyâ€‘driven searches (MTIs, thermoelectrics, multiferroics, superconductors, photocatalystsâ€¦).

---

## ğŸ—‚ï¸ Repository Structure

```
MagMatLLM/
â”œâ”€ configs/                 # YAML configs (surrogate weights, thresholds, objectives)
â”œâ”€ data/
â”‚  â”œâ”€ raw/                  # Raw pulls (TMM, MP)
â”‚  â”œâ”€ augmented/            # Augmented structures (AugLiChem outputs)
â”‚  â””â”€ candidates/           # LLM candidates (JSON/MCIF/POSCAR)
â”œâ”€ docs/
â”‚  â””â”€ figures/              # Pipeline, bands, DOS, phonons, etc.
â”œâ”€ scripts/
â”‚  â”œâ”€ 00_prepare_dataset.py # Data crawl & cleaning (TMM+MP+AugLiChem)
â”‚  â”œâ”€ 10_llm_generate.py    # LLM generation & mutations
â”‚  â”œâ”€ 20_surrogate_eval.py  # CHGNet screening (E_form/E_hull/bulk/M_tot)
â”‚  â”œâ”€ 30_selection.py       # Scoring, ranking, and parent update
â”‚  â”œâ”€ 40_dft_preprocess.py  # DFT preâ€‘processing: supercells/spins/paths
â”‚  â””â”€ 50_analyze_dft.py     # Parse phonons/bands/DOS & export figures
â”œâ”€ src/
â”‚  â”œâ”€ magmatllm/            # Core: prompts, GA, metrics, I/O
â”‚  â””â”€ utils/                # Units, deduplication, geometry checks
â”œâ”€ notebooks/               # Reproduction & visualization
â”œâ”€ LICENSE
â””â”€ README.md
```

---

## âš™ï¸ Installation

```bash
# 1) Create environment (Python 3.10+)
conda create -n magmatllm python=3.10 -y
conda activate magmatllm

# 2) Install dependencies
pip install -r requirements.txt
# Required: pymatgen, chgnet, ase, phonopy, numpy, pandas, torch, pydantic, ruamel.yaml, tqdm

# 3) Optional: GPU acceleration (CUDA/A100 recommended)
```

> **Tip:** On Google Colab, enable a GPU (A100 preferred) and use the demo notebooks in `notebooks/`.

---

## ğŸ”‘ API Keys

* **Materials Project** (for property enrichment):

  ```bash
  export MP_API_KEY=YOUR_KEY
  ```

---

## ğŸ§ª Quickstart

### 1) Prepare datasets

```bash
python scripts/00_prepare_dataset.py \
  --tmm_url https://topologicalquantumchemistry.com/magnetic/index.html \
  --save_dir data/raw \
  --augment true --augment_ops rotate,swap,supercell,defect \
  --out data/augmented
```

### 2) LLM generation (composition/mutation)

```bash
python scripts/10_llm_generate.py \
  --parents data/augmented/parents.json \
  --template configs/prompt_template.yaml \
  --model llama-8b  # or llama-70b for more capacity \
  --n_children 200 --seed 42 \
  --out data/candidates/gen01
```

### 3) Surrogate screening & selection

```bash
python scripts/20_surrogate_eval.py \
  --in_dir data/candidates/gen01 \
  --chgnet relax --props e_form,e_hull,bulk,magmom \
  --out results/gen01_eval.csv

python scripts/30_selection.py \
  --scores results/gen01_eval.csv \
  --objective weighted_sum \
  --alpha_e 1.0 --alpha_b 0.5 --alpha_m 0.5 \
  --keep_top 128 --parent_k 16 --parents_per_group 8 \
  --next_parents data/candidates/parents_gen02.json
```

### 4) DFT pre/postâ€‘processing

```bash
# Generate DFT inputs (e.g., QE/VASP)
python scripts/40_dft_preprocess.py --cands results/gen01_eval.csv --out dft/inputs

# After runs finish, parse phonons/bands/DOS
python scripts/50_analyze_dft.py --dft_out dft/outputs --fig_dir docs/figures
```

---

## ğŸ¯ Objective Functions

* **Weightedâ€‘Sum** (smooth tradeâ€‘off):
  $O_	ext{ws}(x) = lpha_E E^*_d(x) + lpha_b\, b(x) + lpha_m\, M_	ext{tot}(x)$
* **Lexicographic with thresholds**: satisfy stability thresholds first, then optimize bulk modulus and magnetizationâ€”useful when feasible solutions are sparse in early iterations.

> Metrics are normalized via winsorization and minâ€“max. â€œHigherâ€‘isâ€‘betterâ€ metrics are inverted to unify the â€œlowerâ€‘isâ€‘betterâ€ ranking semantics.

---

## ğŸ“Š Reproducing Our Results

* **Baseline**: MatLLMSearch (LLM+GA).
* **This work**: MagMatLLM (adds magnetic and bandâ€‘gap objectives).

Run `notebooks/reproduce_table1.ipynb` to regenerate summary tables and plots, including:

* Candidate counts, surrogate stability rate, DFT pass rate;
* Representative materialsâ€™ phonons/bands/DOS;
* Objective/ablation sensitivity.

> Note: DFT calculations require your own licenses/resources (VASP/QE/ABINIT, etc.).

---

## ğŸ“ Data Sources

* **Topological Magnetic Materials (TMM) DB**: seeds for magnetic/topological compounds.
* **Materials Project (MP)**: PBE band gaps, bulk moduli, E\_hull, etc.
* **AugLiChem**: structureâ€‘level augmentation to diversify sparse regimes.

Please follow each data sourceâ€™s terms and citation requirements.

---

## ğŸ“Œ Highlights

* **New candidates**: tens of previously unreported structures discovered under constrained compute.
* **Validation**: finite band gaps, nonâ€‘zero total magnetic moments, and no imaginary phonon frequencies for exemplars.
* **Efficiency**: 8B LLM + single A100 achieves competitive throughput and candidate quality versus much larger models.

See `docs/figures/` and the paper for details.

---

## ğŸ› ï¸ Roadmap

* Incorporate **topological indicators** (symmetry indicators, Wilsonâ€‘loop proxies, inversion confidence) into objectives/filters.
* Fuse **retrievalâ€‘augmented prompts** with **physics priors** (Wyckoff occupations, layered heavyâ€‘element magnets, prototypes).
* Multiâ€‘fidelity **active learning** to further reduce DFT budget.

---

## ğŸ™‹ FAQ

**Q1: Can I run without multiâ€‘GPU A100s?**
Yes. An 8B model on a single A100 / highâ€‘end workstation can complete multiple generateâ€‘screenâ€‘select cycles; offload the DFT stage to a queue or external cluster if needed.

**Q2: Do I have to use CHGNet?**
No. You can swap in M3GNet/othersâ€”adjust interfaces and normalization in `configs/`.

**Q3: Whatâ€™s the working definition of â€œmagnetic insulatorâ€ here?**
We require **M\_tot > 0** and **E\_g â‰¥ 0.025 eV**, plus phonon stability as final confirmation.

---

## ğŸ¤ Contributing

PRs/issues are welcome, especially for:

* New prompt templates and generation operators (permutation/crossover/distortion);
* Robust deduplication and geometry validity;
* Surrogates and objective design;
* DFT workflows and visualization scripts.

Please see `CONTRIBUTING.md` (coming soon).

---

## ğŸ“š Citation

If this repository helps your research, please cite:

```bibtex
@inproceedings{magmatllm2025,
  title     = {Accelerated Discovery of Magnetic Insulators via LLM-Guided Genetic Algorithm and First-Principles Validation},
  booktitle = {NeurIPS},
  year      = {2025},
  note      = {preprint}
}
```

---

## ğŸ“œ License

Released under the MIT License. See [LICENSE](LICENSE).

---

## ğŸ“ Disclaimer

This codebase and predictions are for research/education only. Synthesis and applications require your own safety and compliance checks.

---

## ğŸ”— Acknowledgements

* Openâ€‘source projects: CHGNet, pymatgen, ASE, phonopy, etc.
* Materials Project and TMM databases.
* AugLiChem for structure augmentation.
* The broader openâ€‘source materialsâ€‘informatics community.
